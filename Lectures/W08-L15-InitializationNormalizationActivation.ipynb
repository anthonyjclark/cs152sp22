{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95b8dd4f",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Things-to-Consider\" data-toc-modified-id=\"Things-to-Consider-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Things to Consider</a></span></li><li><span><a href=\"#Synthetic-Input\" data-toc-modified-id=\"Synthetic-Input-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Synthetic Input</a></span></li><li><span><a href=\"#Fully-Connected-Neural-Network-With-Linear-Output\" data-toc-modified-id=\"Fully-Connected-Neural-Network-With-Linear-Output-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Fully-Connected Neural Network With Linear Output</a></span></li><li><span><a href=\"#Training-Loop\" data-toc-modified-id=\"Training-Loop-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Training Loop</a></span></li><li><span><a href=\"#Examine-Hidden-Calculations\" data-toc-modified-id=\"Examine-Hidden-Calculations-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Examine Hidden Calculations</a></span></li><li><span><a href=\"#Model-Initialization-and-Normalization\" data-toc-modified-id=\"Model-Initialization-and-Normalization-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Model Initialization and Normalization</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14027b",
   "metadata": {},
   "source": [
    "## Things to Consider\n",
    "\n",
    "1. The purpose of activation functions.\n",
    "1. The benefits of with and depth.\n",
    "1. Causes of problematic gradients (vanishing and exploding).\n",
    "1. Proper parameter initialization.\n",
    "1. Normalization (input and between layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30c6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from fastprogress.fastprogress import master_bar, progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde7614d",
   "metadata": {},
   "source": [
    "## Synthetic Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0394377",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500\n",
    "X = torch.linspace(-3, 3, N).reshape(-1, 1)\n",
    "y = torch.sin(X)\n",
    "_ = plt.plot(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f86b3e",
   "metadata": {},
   "source": [
    "## Fully-Connected Neural Network With Linear Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02170aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # Hidden layers\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # Output layer\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee01eb2",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcedc33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X, y, num_epochs=2000):\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    for epoch in progress_bar(range(num_epochs)):\n",
    "        yhat = model(X)\n",
    "        loss = criterion(yhat, y)\n",
    "        losses.append(loss.item())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses\n",
    "\n",
    "\n",
    "# Compare: width vs depth\n",
    "layer_sizes = (1, 8, 1)\n",
    "model = NeuralNetwork(layer_sizes)\n",
    "summary(model)\n",
    "losses = train(model, X, y)\n",
    "print(f\"Final loss: {losses[-1]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f964a47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model(X)\n",
    "\n",
    "_, (ax1, ax2) = plt.subplots(2, 1)\n",
    "\n",
    "ax1.plot(X, y)\n",
    "ax1.plot(X, yhat.detach())\n",
    "_ = ax2.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df5b149",
   "metadata": {},
   "source": [
    "## Examine Hidden Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e19f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_input = None\n",
    "\n",
    "\n",
    "def capture_final_outputs(module, layer_in, layer_out) -> None:\n",
    "    global final_layer_input\n",
    "    final_layer_input = layer_in[0].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9242f683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hook to capture input to final layer (if not already registered)\n",
    "if final_layer_input == None:\n",
    "    final_layer = model.layers[-1]\n",
    "    final_layer.register_forward_hook(capture_final_outputs)\n",
    "\n",
    "# Compute model output and capture input to final layer\n",
    "# X = torch.linspace(10, 20, 100).reshape(-1, 1)\n",
    "yhat = model(X)\n",
    "\n",
    "# Grab parameters for the output layer\n",
    "WL = list(model.parameters())[-2].detach()\n",
    "bL = list(model.parameters())[-1].item()\n",
    "\n",
    "# Plot each input to the final layer\n",
    "plt.plot(X, final_layer_input * WL, label=\"Activation\")\n",
    "\n",
    "# Plot the output of the final layer\n",
    "plt.plot(X, yhat.detach(), \"o\", label=\"yhat\")\n",
    "\n",
    "# Compare with hand-computed final layer output\n",
    "plt.plot(X, final_layer_input @ WL.T + bL, \"--\", label=\"Combined Activations\")\n",
    "\n",
    "_ = plt.legend(bbox_to_anchor=(1.04,1), loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d5717b",
   "metadata": {},
   "source": [
    "## Model Initialization and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75d42886",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(layer):\n",
    "    if type(layer) == torch.nn.Linear:\n",
    "        print(\"Initializing\", layer)\n",
    "\n",
    "        if kind == \"zeros\":\n",
    "            layer.weight.data.fill_(0.0)\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "        elif kind == \"ones\":\n",
    "            layer.weight.data.fill_(1.0)\n",
    "            layer.bias.data.fill_(1.0)\n",
    "\n",
    "        elif kind == \"uniform\":\n",
    "            layer.weight.data.uniform_()\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "        elif kind == \"normal\":\n",
    "            layer.weight.data.normal_()\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "        elif kind == \"normal2\":\n",
    "            layer.weight.data.normal_() * (1 / torch.sqrt(torch.tensor(layer.weight.shape[0])))\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "        elif kind == \"xavier\":\n",
    "            torch.nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "        elif kind == \"kaiming\":\n",
    "            torch.nn.init.kaiming_normal_(layer.weight)\n",
    "\n",
    "        else:\n",
    "            print(f\"'{kind}' is not handled\")\n",
    "\n",
    "\n",
    "layer_sizes = (1, 10, 10, 10, 1)\n",
    "\n",
    "model = NeuralNetwork(layer_sizes)\n",
    "kind = \"zeros\"\n",
    "model.apply(init_weights)\n",
    "\n",
    "losses = train(model, X, y)\n",
    "print(f\"Final loss: {losses[-1]:.3f}\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    A = X\n",
    "    std, mean = torch.std_mean(A)\n",
    "    print(f\"\\n*** A0: Mean = {mean.item():.3f}, STD = {std.item():.3f}\\n\")\n",
    "    for l, layer in enumerate(model.layers):\n",
    "        print(layer)\n",
    "        A = layer(A)\n",
    "        std, mean = torch.std_mean(A)\n",
    "        print(f\"\\n*** A{l+1}: Mean = {mean.item():.3f}, STD = {std.item():.3f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a3b30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
