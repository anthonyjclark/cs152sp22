{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e830987",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Questions-to-Answer\" data-toc-modified-id=\"Questions-to-Answer-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Questions to Answer</a></span></li><li><span><a href=\"#Imports\" data-toc-modified-id=\"Imports-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Imports</a></span></li><li><span><a href=\"#Create-Fake-Training-Data\" data-toc-modified-id=\"Create-Fake-Training-Data-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Create Fake Training Data</a></span></li><li><span><a href=\"#Utilities\" data-toc-modified-id=\"Utilities-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Utilities</a></span></li><li><span><a href=\"#Train-a-Linear-Model-Using-Batch-Gradient-Descent\" data-toc-modified-id=\"Train-a-Linear-Model-Using-Batch-Gradient-Descent-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Train a Linear Model Using Batch Gradient Descent</a></span></li><li><span><a href=\"#Train-a-Quadratic-Model-Using-Batch-Gradient-Descent\" data-toc-modified-id=\"Train-a-Quadratic-Model-Using-Batch-Gradient-Descent-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Train a Quadratic Model Using Batch Gradient Descent</a></span></li><li><span><a href=\"#Train-a-Cubic-Model-Using-Batch-Gradient-Descent\" data-toc-modified-id=\"Train-a-Cubic-Model-Using-Batch-Gradient-Descent-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Train a Cubic Model Using Batch Gradient Descent</a></span></li><li><span><a href=\"#Train-a-Polynomial-Model-Using-Batch-Gradient-Descent\" data-toc-modified-id=\"Train-a-Polynomial-Model-Using-Batch-Gradient-Descent-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Train a Polynomial Model Using Batch Gradient Descent</a></span></li><li><span><a href=\"#Compute-Polynomial-Model-Using-Ordinary-Least-Squares\" data-toc-modified-id=\"Compute-Polynomial-Model-Using-Ordinary-Least-Squares-9\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Compute Polynomial Model Using Ordinary Least Squares</a></span></li><li><span><a href=\"#Train-Neural-Network-Model-Using-Batch-Gradient-Descent\" data-toc-modified-id=\"Train-Neural-Network-Model-Using-Batch-Gradient-Descent-10\"><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Train Neural Network Model Using Batch Gradient Descent</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633ff60f",
   "metadata": {},
   "source": [
    "# Overfitting a Curve\n",
    "\n",
    "In this assignment, you will play around with the various models and corresponding parameters. \n",
    "\n",
    "## Questions to Answer\n",
    "\n",
    "Things to try:\n",
    "\n",
    "- **Before you run any code**, make some predictions. What do you expect to see for the different models?\n",
    "    + linear\n",
    "    + quadratic\n",
    "    + cubic\n",
    "    + $n$ degree polynomial\n",
    "    + neural network\n",
    "- Now run the notebook. What surprised you?\n",
    "- Now report on your results with the following:\n",
    "    + Changing the number of degrees in the polynomial model.\n",
    "    + Using a non-zero weight decay for the neural network model.\n",
    "    + Changing the number of layers in the neural network model.\n",
    "    + Changing the number of training samples.\n",
    "- Finally, open the [second notebook for this assignment](https://github.com/anthonyjclark/cs152sp22/blob/main/Assignments/A08-Overfitting/OverfittingFashionMNIST.ipynb) and see if you can get the neural network to overfit the data (get the bad thing to happen)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3686a479",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e0108a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from fastprogress.fastprogress import progress_bar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from jupyterthemes import jtplot\n",
    "\n",
    "jtplot.style(context=\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc2ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training examples\n",
    "train_N = 20\n",
    "\n",
    "# Range of training data input\n",
    "MIN_X, MAX_X = -1, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605dd3de",
   "metadata": {},
   "source": [
    "## Create Fake Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fake_y(x, add_noise=False):\n",
    "    y = 10 * x ** 3 - 5 * x\n",
    "    return y + torch.randn_like(y) * 0.5 if add_noise else y\n",
    "\n",
    "\n",
    "true_N = 100\n",
    "true_X = torch.linspace(MIN_X, MAX_X, true_N)\n",
    "true_y = fake_y(true_X)\n",
    "\n",
    "train_X = torch.rand(train_N) * (MAX_X - MIN_X) + MIN_X\n",
    "train_y = fake_y(train_X, add_noise=True)\n",
    "\n",
    "plt.scatter(train_X, train_y, label=\"Noisy Samples\")\n",
    "plt.plot(true_X, true_y, \"--\", label=\"True Function\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ba9b6",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_curves(ax, model=None, MSE=None, poly_deg=0):\n",
    "\n",
    "    # Plot the noisy scatter points and the \"true\" function\n",
    "    ax.scatter(train_X, train_y, label=\"Noisy Samples\")\n",
    "    ax.plot(true_X, true_y, \"--\", label=\"True Function\")\n",
    "\n",
    "    # Plot the model's learned regression function\n",
    "    if model:\n",
    "        x = true_X.unsqueeze(-1)\n",
    "        x = x.pow(torch.arange(poly_deg + 1)) if poly_deg else x\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred_y = model(x)\n",
    "\n",
    "        ax.plot(true_X, pred_y, label=\"Learned Curve\")\n",
    "\n",
    "    ax.set_xlim([MIN_X, MAX_X])\n",
    "    ax.set_ylim([-5, 5])\n",
    "    ax.legend()\n",
    "    if MSE:\n",
    "        ax.set_title(f\"MSE = ${MSE:.5f}$\")\n",
    "\n",
    "\n",
    "def plot(model=None, losses=None, poly_deg=0):\n",
    "\n",
    "    if losses:\n",
    "        _, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    else:\n",
    "        _, ax1 = plt.subplots(1, 1, figsize=(8, 8))\n",
    "\n",
    "    loss = losses[-1] if losses else None\n",
    "    plot_curves(ax1, model, MSE=loss, poly_deg=poly_deg)\n",
    "\n",
    "    if losses:\n",
    "        ax2.plot(losses)\n",
    "        ax2.set_xlabel(\"Epoch\")\n",
    "        ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "\n",
    "def train_model(X, y, lr, ne, wd, model, params):\n",
    "    # Hyperparameters\n",
    "    learning_rate = lr\n",
    "    num_epochs = ne\n",
    "\n",
    "    # Torch utils\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(params, lr=learning_rate, weight_decay=wd)\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in progress_bar(range(num_epochs)):\n",
    "        # Model\n",
    "        yhat = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(yhat.squeeze(), y)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5bb598",
   "metadata": {},
   "source": [
    "## Train a Linear Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf31e5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "m = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Place parameters into a sequence for torch.optim\n",
    "params = (b, m)\n",
    "\n",
    "# Create simple linear model\n",
    "def model(X):\n",
    "    return m * X + b\n",
    "\n",
    "\n",
    "losses = train_model(\n",
    "    train_X, train_y, learning_rate, num_epochs, weight_decay, model, params\n",
    ")\n",
    "plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee9b37f",
   "metadata": {},
   "source": [
    "## Train a Quadratic Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd011eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 64\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "w2 = torch.randn(1, requires_grad=True)\n",
    "w1 = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Place parameters into a sequence for torch.optim\n",
    "params = (b, w1, w2)\n",
    "\n",
    "# Create simple quadratic model\n",
    "def model(X):\n",
    "    return b + w1 * X + w2 * X ** 2\n",
    "\n",
    "\n",
    "losses = train_model(\n",
    "    train_X, train_y, learning_rate, num_epochs, weight_decay, model, params\n",
    ")\n",
    "plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db198adc",
   "metadata": {},
   "source": [
    "## Train a Cubic Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6dfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "w3 = torch.randn(1, requires_grad=True)\n",
    "w2 = torch.randn(1, requires_grad=True)\n",
    "w1 = torch.randn(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# Place parameters into a sequence for torch.optim\n",
    "params = (b, w1, w2)\n",
    "\n",
    "# Create simple cubic model\n",
    "def model(X):\n",
    "    return b + w1 * X + w2 * X ** 2 + w3 * X ** 3\n",
    "\n",
    "\n",
    "losses = train_model(\n",
    "    train_X, train_y, learning_rate, num_epochs, weight_decay, model, params\n",
    ")\n",
    "plot(model, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44575ec",
   "metadata": {},
   "source": [
    "## Train a Polynomial Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58bd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "weight_decay = 0\n",
    "\n",
    "# Model parameters\n",
    "degrees = 50  # 3, 4, 16, 32, 64, 128\n",
    "powers = torch.arange(degrees + 1)\n",
    "x_poly = train_X.unsqueeze(-1).pow(powers)\n",
    "params = torch.randn(degrees + 1, requires_grad=True)\n",
    "\n",
    "# Create simple cubic model\n",
    "def model(X):\n",
    "    return X @ params\n",
    "\n",
    "\n",
    "losses = train_model(x_poly, train_y, learning_rate, num_epochs, weight_decay, model, [params])\n",
    "plot(model, losses, poly_deg=degrees)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3663def1",
   "metadata": {},
   "source": [
    "## Compute Polynomial Model Using Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f9ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute \"optimal\" parameters\n",
    "params = ((x_poly.T @ x_poly).inverse() @ x_poly.T) @ train_y\n",
    "\n",
    "\n",
    "def model(X):\n",
    "    return X @ params\n",
    "\n",
    "\n",
    "# Compute loss\n",
    "mse = nn.functional.mse_loss(x_poly @ params, train_y)\n",
    "\n",
    "plot(model, [mse], poly_deg=degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3feed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.abs().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d5fbc",
   "metadata": {},
   "source": [
    "## Train Neural Network Model Using Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099a5473",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        # The hidden layers include:\n",
    "        # 1. a linear component (computing Z) and\n",
    "        # 2. a non-linear comonent (computing A)\n",
    "        hidden_layers = [\n",
    "            nn.Sequential(nn.Linear(nlminus1, nl), nn.ReLU())\n",
    "            for nl, nlminus1 in zip(layer_sizes[1:-1], layer_sizes)\n",
    "        ]\n",
    "\n",
    "        # The output layer must be Linear without an activation. See:\n",
    "        #   https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html\n",
    "        output_layer = nn.Linear(layer_sizes[-2], layer_sizes[-1])\n",
    "\n",
    "        # Group all layers into the sequential container\n",
    "        all_layers = hidden_layers + [output_layer]\n",
    "        self.layers = nn.Sequential(*all_layers)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ab3191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10000\n",
    "weight_decay = 0\n",
    "\n",
    "layer_sizes = (1, 100, 100, 100, 1)\n",
    "\n",
    "model = NeuralNetwork(layer_sizes)\n",
    "summary(model)\n",
    "\n",
    "X = train_X.unsqueeze(-1)\n",
    "\n",
    "losses = train_model(X, train_y, learning_rate, num_epochs, weight_decay, model, model.parameters())\n",
    "plot(model, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9927f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.abs().mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25feac7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
